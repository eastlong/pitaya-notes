## 一、资源配置调优



​		Flink性能调优的第一步，就是为任务分配合适的资源，在一定范围内，增加资源的分配与性能的提升是成正比的，实现了最优的资源配置后，在此基础上再考虑进行后面论述的性能调优策略。          

​		提交方式主要是`yarn-per-job`，资源的分配在使用脚本提交Flink任务时进行指定。标准的Flink任务提交脚本（Generic CLI 模式）从1.11开始，增加了通用客户端模式，参数使用-D 指定。

```bash
bin/flink run
-t yarn-per-job
-d
-p 5 \ 指定并行度
-Dyarn.application.queue=test \ 指定yarn队列
-Djobmanager.memory.process.size=1024mb \ 指定JM的总进程大小
-Dtaskmanager.memory.process.size=1024mb \ 指定每个TM的总进程大小
-Dtaskmanager.numberOfTaskSlots=2 \ 指定每个TM的slot数
-c com.atguigu.app.dwd.LogBaseApp
/opt/module/gmall-flink/gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar
```



### 1.1 内存配置

```bash
bin/flink run
-t yarn-per-job
-d
-p 5 \ 指定并行度
-Dyarn.application.queue=test \ 指定yarn队列
-Djobmanager.memory.process.size=2048mb \ JM2<sub>4G足够
-Dtaskmanager.memory.process.size=6144mb \ 单个TM2</sub>8G足够
-Dtaskmanager.numberOfTaskSlots=2 \ 与容器核数1core：1slot或1core：2slot
-c com.atguigu.app.dwd.LogBaseApp
/opt/module/gmall-flink/gmall-realtime-1.0-SNAPSHOT-jar-with-dependencies.jar
```

 Flink是实时流处理，关键在于资源情况能不能抗住高峰时期每秒的数据量，通常用QPS/TPS来描述数据情况。





### 1.1 生产上常用的Flink配置参数

- JobManagerMemory JM的内存大小
- TaskManagerMemory  TM的内存大小
- TMSlot task slot的数量
- MaxPar 最大并行度





## 二、反压处理

​		反压（BackPressure）通常产生于这样的场景：**短时间的负载高峰导致系统接收数据的速率远高于它处理数据的速率**。许多日常问题都会导致反压，例如，**垃圾回收停顿**可能会导致流入的数据快速堆积，或遇到大促、秒杀活动导致流量陡增。反压如果不能得到正确的处理，可能会导致资源耗尽甚至系统崩溃。 反压机制是指系统能够自己检测到被阻塞的 Operator，然后自适应地降低源头或上游数据的发送速率，从而维持整个系统的稳定。Flink 任务一般运行在多个节点上，数据从上游算子发送到下游算子需要网络传输，若系统在反压时想要降低数据源头或上游算子数据的发送速率，那么肯定也需要网络传输。所以下面先来了解一下 Flink 的网络流控（Flink 对网络数据流量的控制）机制。

### 2.1 反压现象与定位

在 Flink Web UI 中有 BackPressure 的页面，通过该页面可以查看任务中 subtask 的反压状态，如下两图所示，分别展示了状态是 OK 和 HIGH 的场景。

![Flink企业级优化全面总结（3万字长文，15张图）_并行度_05](images/resize,m_fixed,w_1184)

![Flink企业级优化全面总结（3万字长文，15张图）_字段_06](images/resize,m_fixed,w_1184)